# 记忆系统构想记录

本文档用于记录关于记忆系统设计的原始表述和构想。

---

## 1. 核心观点

### 1.1 记忆的本质

记忆是**记忆节点的超链接网络**。

- 每个节点包含内容以及与其他记忆节点的关联关系
- 网络结构是记忆的本质形态，节点通过超链接相互连接
- 记忆的价值不在于孤立存储，而在于网络中的连接关系

### 1.2 与现有方案的区别

| 特性 | 本方案 | 向量数据库+RAG | MemGPT | 传统记忆系统 |
|------|--------|----------------|--------|--------------|
| **核心结构** | 超链接网络（节点+关联） | 孤立向量 | 分层存储 | 键值对/列表 |
| **检索方式** | BFS从关注点向外搜索 | 全局相似度匹配 | 固定窗口+分层检索 | 精确匹配/最近访问 |
| **遗忘机制** | 渐进压缩+关联断裂 | 无遗忘，永久存储 | 压缩归档或删除 | TTL过期/固定容量淘汰 |
| **保护机制** | 重要性（关联强度之和） | 无 | 核心上下文固定保留 | 无 |
| **动态保护** | 关注点机制（LRU 5个） | 无 | 无 | 无 |
| **时间度量** | 压缩循环次数+扫描次数 | 无时间概念 | 无时间概念 | 物理时间/TTL |
| **关联衰减** | 每次压缩衰减 | 无关联 | 无关联 | 无关联 |
| **搜索起点** | 当前关注点 | 全库 | 最近上下文 | 全库/固定范围 |
| **压缩方式** | 语义压缩（小模型） | 无压缩 | 摘要压缩 | 删除/截断 |
| **长期记忆** | 扫描次数累积（访问少=保留久） | 无区分 | 外部存储 | 备份机制 |

#### 关键差异说明

**1. 网络结构 vs 孤立存储**
- 本方案：记忆节点通过关联形成网络，检索时沿着关联传播
- 向量库：每个记忆是独立向量，检索时计算与查询的相似度
- 优势：网络结构模拟人类记忆的联想特性，相关记忆自然聚集

**2. 渐进压缩 vs 二元删除**
- 本方案：内容随重要性降低而逐步压缩，直到字数<10才删除
- 传统方案：要么保留完整内容，要么直接删除
- 优势：避免信息突然丢失，给记忆"淡出"的过程

**3. 关注点动态保护**
- 本方案：当前讨论相关的记忆自动成为关注点，不受压缩
- 其他方案：缺乏对"当前相关"的动态识别和保护
- 优势：确保对话连贯性，不会被意外遗忘打断

**4. 扫描次数=长期记忆**
- 本方案：被频繁扫描的记忆扫描次数增加，后续被扫描间隔变长
- 其他方案：要么固定保留，要么按时间淘汰
- 优势：真正常用的记忆形成"长期记忆"，不常用的逐渐被压缩

**5. 重要性驱动 vs 相似度驱动**
- 本方案：重要性=关联强度之和，决定是否压缩
- RAG方案：相似度=向量距离，决定是否召回
- 本质区别：重要性是节点的"保护力"，相似度是查询的"匹配度"

**6. 时间作为过程而非参数**
- 本方案：时间不参与计算，时间流逝体现为"进行了多少次压缩"
- 其他方案：通常使用物理时间（TTL、时间戳）
- 优势：遗忘速度与系统活跃度相关，而非真实时间

---

## 2. 节点结构

每个记忆节点包含以下要素：

| 要素 | 说明 | 变化趋势 |
|------|------|----------|
| **内容** | 记忆的完整内容 | 随重要性逐步缩减 |
| **短语** | 内容的简短概括 | 随内容缩减而减少 |
| **关键词** | 用于关联和检索的标签 | 随内容缩减而减少 |
| **时间** | 记忆创建时间 | 元数据，仅记录 |
| **扫描次数** | 被压缩扫描的次数 | 每次扫描+1 |

### 2.1 内容的语义压缩

节点内容持续经历压缩：

```
完整内容
    │
    ▼ 0 < 重要性 < 1
内容按比例保留（目标字数 = min(重要性,1) × 原文）
    │
    ▼ 重要性降低（关联衰减）
保留字数减少
    │
    ▼ 重要性 = 0 或 字数 < 10
遗忘（节点从网络中移除）
```

**关键机制**：
- 压缩是**连续的、渐进的**，而非离散的步骤
- 重要性由数学公式决定，而非if-else判断
- 重要性：**所有关联强度之和，[0, ∞)**
  - 重要性 = 0：保留0%（直接删除节点/遗忘）
  - 0 < 重要性 < 1：保留 `重要性` 比例的内容
  - 重要性 ≥ 1：保留100%（不压缩，受到保护）

---

## 3. 重要性公式

### 3.1 核心公式

重要性由以下因素**连续计算**得出：

```
重要性 = 所有关联强度之和
```

**输入变量**：

| 变量 | 符号 | 说明 | 取值范围 |
|------|------|------|----------|
| 关联强度列表 | [s₁, s₂, ...] | 该节点所有关联的强度值列表 | 每个 sᵢ ∈ [0, 1] |

**计算方式**：
```
重要性 = Σ sᵢ  (所有关联强度之和)
```

**说明**：
- 当前为简化定义，直接求和
- 取值范围：[0, ∞)，无上界
- 后续可能添加激活函数映射到 [0, 1]

**时间的作用**：
- 时间是**元数据**，仅记录记忆创建的时间点
- **不参与重要性计算**
- 时间流逝的效果通过以下机制自然体现：
  - 大批节点持续压缩 → 内容退化 → 关联断裂
  - 节点被遗忘 → 其关联的其他节点失去支撑 → 连锁遗忘
  - 网络整体的演化本身就蕴含了时间流逝

**公式特性**：
- 所有关联强度**同时参与**计算
- 重要性与连接强度**正相关**（关联越多越强，越不容易被遗忘）
- 取值范围：[0, ∞)

### 3.2 压缩的必然性：过程即时间

**核心洞察**：

只要 **0 < 重要性 < 1**，节点就会被压缩。重要性越低，保留的内容越少。

```
重要性 = 0.5
    │
    ▼ 每次系统循环
保留50%内容
    │
    ▼ 关联强度衰减 → 重要性降低
保留内容比例下降
    │
    ▼ 持续进行
重要性 → 0（关联全部断裂）→ 遗忘
```

**这就是时间的力量**：

- 不需要显式的"时间变量"
- 不需要计算"经过了多少秒"
- **持续的压缩过程本身就是时间的度量**
- 每个压缩循环 = 一个时间单位

**阻止压缩（保留全部内容）的唯一方式**：

重要性 ≥ 1

这需要：
- 至少一个强度为1的关联（如与关注点关联）

在实际情况中，这些条件难以持续满足，因此**遗忘是必然的**。

### 3.3 动态平衡与必然遗忘

**压缩的必然性**：

只要 **0 < 重要性 < 1**，压缩就进行；每次压缩关联强度衰减，重要性降低，最终重要性=0时删除节点（遗忘）。

新建立的关联可以暂时提升重要性，但随着时间推移（多次压缩），遗忘是必然的。

这是一个**动态平衡系统**，新输入可以延缓但无法阻止遗忘。

### 3.4 遗忘条件

```
重要性 = 0  →  节点被遗忘（从网络中移除）
```

或（可选）：

```
所有关联强度 = 0  →  节点孤立 → 遗忘
```

---

## 4. 关联关系

节点之间的连接包含：

| 属性 | 说明 | 变化趋势 |
|------|------|----------|
| **强度** | 连接的紧密程度 | 随时间自然衰减 |
| **关系名称** | 关联的语义类型 | 随内容压缩可能丢失 |

### 4.1 关联强度的动态变化

- 新建立时：初始强度 = `link_initial_strength`（默认 0.5）
- 时间推移：自然衰减（每次压缩时乘以 decay_rate）

#### 关联强度衰减机制

**关联衰减速率**（`decay_rate`）：每次压缩时的强度衰减系数

| 属性 | 说明 |
|------|------|
| 取值范围 | (0, 1] |
| 典型值 | 0.95 ~ 0.99（每次压缩衰减1%~5%） |
| 计算方式 | 新强度 = 旧强度 × decay_rate |

**节点删除时机**：重要性 = 0 时，直接删除节点

```
压缩节点
    │
    ▼ 计算重要性 = Σ关联强度
    │
    ├─ 重要性 ≥ 1 → 受保护，跳过
    │
    ├─ 0 < 重要性 < 1 → 执行压缩
    │
    └─ 重要性 = 0 → 直接删除节点（不处理关联）
```

**关联的懒删除与"记得曾经记得"**：
- 节点删除时**不主动清理**指向它的关联
- 悬空关联（目标节点已不存在的关系）表达了**"曾经知道但已遗忘"**的概念
- 智能体可以通过感知悬空关联，意识到自己曾经记得某事，但现在忘记了
- 这种设计赋予了遗忘以**可感知性**，而非彻底的虚无

### 4.2 关联的形成机制（概念视角）

记忆系统中的**关联不需要主动声明**，而是通过记忆的自然积累过程**自发形成**。

#### 关联的自发形成

**1. 时间顺序关联（蕴含形成）**
```
用户讨论话题A → 创建节点A（成为关注点）
    │
用户继续讨论话题B → 创建节点B
    │
    ▼ 节点B与当前关注点（节点A）建立关联
    
时间链条自然形成：A ↔ B ↔ C ↔ ...
```

时间顺序不需要显式的时间戳记录，而是通过**与当前关注点的关联**自然蕴含。新记忆与当前活跃的记忆建立强关联，形成时间流。

**2. 语义连贯关联（切割形成）**
```
长文本输入
    │
语义切割 → [小段1, 小段2, 小段3]
    │
    ▼ 段间关联
小段1 ↔ 小段2 ↔ 小段3（强度0.5）
```

同一来源的多个记忆片段之间自动建立关联，保持原始文本的语义连贯性。

**3. 隐含关联（相似形成）**
```
用户今天说："我喜欢吃苹果"
    ↓ 创建节点A

用户明天说："我喜欢吃苹果"
    ↓ 创建节点B（内容相似的副本）
    
节点A和节点B都关联到"苹果"这个关注点
    ↓
通过共享关注点，两个节点形成隐含关联
```

当用户多次讨论相似内容时，系统创建**内容相似的节点副本**。这些副本通过**共享的关注点**自然形成关联网络。旧副本被遗忘后，新副本仍然保留，确保重要信息不会因为单次遗忘而丢失。

#### 副本独立性原则

每个记忆节点都是**独立的实体**：
- 节点A和节点B即使有相似内容，也是两个独立的记忆
- 节点A被遗忘，不影响节点B的存在
- 这种"冗余"设计确保重要记忆的存活率

#### 关联与检索的关系

关联不是为存储而存在，而是为检索而存在：
- **存储时**：关联自然形成，无需人工干预
- **检索时**：通过关联网络（BFS）发现相关记忆
- 关联强度决定检索时的路径优先级

### 4.3 节点的遗忘与关联的清理

**遗忘条件**：
```
重要性 = 0  →  直接删除节点（无需处理关联）
```

**关联的懒删除机制**：

```
节点A → 关联 → [节点B（已删除）]
    │
    ▼ 从A出发遍历关联时
    │
    ▼ 发现节点B不存在
    │
    ▼ 自动删除A到B的关联
```

**为什么采用懒删除？**

| 方案 | 问题 | 本方案（懒删除）|
|------|------|----------------|
| 级联删除 | 删除节点时需遍历所有关联，O(n)复杂度 | 删除节点时直接删除，O(1)复杂度 |
| 预清理 | 增加压缩流程的复杂度和耗时 | 压缩流程只处理节点，不处理关联 |
| 引用计数 | 需要维护计数器，增加写入开销 | 无额外写入开销 |

**"记得曾经记得"——悬空关联的语义价值**：

悬空关联（dangling link）不是需要清理的垃圾，而是具有独立语义的记忆痕迹：

```
节点A → 关联 → [节点B（已删除）]
    │
    └─ 表达了：A曾经与B有关联，但B已被遗忘
```

这种结构使智能体能够：
1. **感知遗忘的存在**：意识到自己曾经知道某事
2. **定位遗忘的位置**：知道在哪个领域失去了记忆
3. **理解自身的知识边界**：明确区分"从未知道"和"曾经知道但忘了"

**关联消失的唯一条件**：

关联只能从**源端**消失，不能从**目标端**消失：

```
源节点被删除          关联强度衰减至断裂阈值
        │                       │
        ▼                       ▼
   关联自然消失            关联断裂消失
```

目标节点的删除不会导致关联消失，只会使其成为悬空关联，保留"曾经记得"的语义。

**关联与压缩的关系**：
- 关联强度衰减 → 重要性降低 → 节点更容易被遗忘
- 节点删除后，指向它的关联变成"悬垂关联"，保留"曾经知道"的痕迹
- 悬垂关联不会被自动清理，只能通过源节点删除或自然衰减消失

---

## 5. 数学化设计原则

### 5.1 连续而非离散

| 离散设计（避免） | 连续设计（采用） |
|----------------|----------------|
| if-else判断 | 数学函数计算 |
| "是/否"遗忘 | 重要性从高到低渐变 |
| "强/中/弱"关联等级 | 强度连续值 [0,1] |
| 阶段性压缩 | 连续比例缩减 |

### 5.2 多因素同时作用

所有影响因素**同时输入公式**，无优先级顺序：

```
最终状态 = Formula(因素A, 因素B, 因素C, ...)
```

而非：

```
if (因素A) {
    if (因素B) {
        ...
    }
}
```

### 5.3 需要确定的具体公式

以下公式需要明确定义：

1. **重要性公式**（核心）：
   ```
   重要性 = Σ sᵢ
   ```
   已确定：
   - 函数形式：所有关联强度之和
   - 取值范围：[0, ∞)
   - 后续可能添加激活函数映射到 [0, 1]

2. **关联强度演化**：
   ```
   sᵢ(t+1) = sᵢ(t) × decay_rate
   ```
   已确定：
   - 新关联初始强度：`link_initial_strength` = 0.5
   - 衰减机制：每次压缩时乘以 `decay_rate`

---

## 6. 记忆检索接口

### 6.1 回忆函数

智能体主动获取记忆内容的接口。

**函数签名**：
```typescript
function recall(
    keywords: string[],      // 关键词列表
    relations: string[],     // 关系列表
    depth: number            // 搜索深度（层数）
): Promise<string>
```

**参数说明**：

| 参数 | 类型 | 必填 | 建议值 | 说明 |
|------|------|------|--------|------|
| `keywords` | `string[]` | 是 | - | 用于匹配记忆节点内容的关键词 |
| `relations` | `string[]` | 是 | - | 用于筛选关联类型的关系名称，空数组表示不筛选 |
| `depth` | `number` | 是 | 2 | 从关注点开始的搜索深度（层数） |

**搜索深度说明**：
| 深度值 | 适用场景 |
|--------|----------|
| 2 | 标准值，日常思考使用 |
| > 2 | 复杂问题思考，扩大搜索范围 |

**返回值**：
- 类型：`Promise<string>`
- 内容：包含所有相关记忆内容的拼接字符串

**异步说明**：
- `recall()` 是异步函数，搜索过程在后台线程执行
- 搜索可能耗时较长（尤其深度较大时）
- 搜索结果通过 Promise 返回给调用线程
- 具体异步管理和线程调度在技术文档中讨论

**使用示例**：
```typescript
// 标准深度（日常思考）
const memory = await recall(
    ["项目", "进度"],
    ["同事", "负责"],
    2
);

// 扩大搜索范围（复杂问题）
const deepMemory = await recall(
    ["架构", "设计"],
    [],
    4
);
```

**搜索策略**：
- **算法**：广度优先搜索（BFS）
- **起点**：当前关注点
- **范围**：从关注点向外扩展 `depth` 层
- **匹配**：在搜索范围内匹配 `keywords` 和 `relations`

**副作用**：
- 调用 `recall()` 会基于 `keywords` 更新智能体的关注点
- 详见[第7节：关注点机制](#7-关注点机制)

---

## 7. 关注点机制

关注是一种保护机制，确保当前讨论相关的记忆不被压缩。

### 7.1 核心概念

**关注点**：被智能体主动标记为"正在关注"的记忆节点。

| 属性 | 说明 |
|------|------|
| 数量上限 | `focus_limit` = 5 |
| 排序依据 | 最后想到的时间（LRU） |
| 保护效果 | 不被压缩、关联强度固定为1 |

### 7.2 关注点的行为

**被关注的节点**：
```
压缩时跳过该节点
关联强度锁定为1（不随衰减计算）
```

**取消关注后**：
```
内容：保持当前状态（不恢复）
关联强度：保持当前值（不再锁定为1）
后续：正常参与压缩、正常衰减
```

### 7.3 关注点的来源

**关注点由回忆函数的关键词计算得出**。

当智能体调用 `recall(keywords, relations)` 时：
```
keywords 参数
    │
    ▼ 提取有效关键词
    │   （名词或代词，指代事物/领域）
    │
    ▼ 映射到相关记忆节点
    │
    ▼ 将节点标记为关注点
```

**关键词要求**：
- 词性：必须是名词或代词
- 语义：指代一个事物、一个领域
- 示例："项目"、"用户系统"、"他"、"前端组"

### 7.4 关注点的更新机制

```
智能体调用 recall()
    │
    ▼ 提取 keywords 中的有效关键词
    │
    ▼ 检查当前关注点数量
    │
    ├─ 数量 < 5 → 直接添加为新关注点
    │
    └─ 数量 = 5 → 移除最久未提及的关注点
                      │
                      ▼ 添加新关注点
```

**淘汰策略**：LRU（Least Recently Used）
- 按照"最后想到的时间"排序
- 新关注点加入时，淘汰最老的

---

## 8. 记忆的建立

### 8.1 触发机制（外部系统）

> **注意**：记忆系统本身不决定何时建立记忆。
> 
> **触发判断由大模型调度系统执行**：
> - 监控上下文长度
> - 当达到配置阈值（如最大上下文长度的80%）时
> - 将超出部分提交给记忆系统处理

**建议配置参数**：
| 参数 | 说明 | 建议值 |
|------|------|--------|
| `context_threshold_ratio` | 上下文阈值比例 | 80% |
| `context_max_tokens` | 最大上下文Token数 | 配置文件指定 |

### 8.2 语义切割

**步骤**：外部系统提交的记忆内容，首先经过小模型的语义切割。

```
原始长文本
    │
    ▼ 调用小模型语义切割
    │   任务：将长文本切分为语义完整的小段
    │   
    ▼ 示例：
    │   输入："我今天去了公园，看到了很多花。然后去了图书馆。"
    │   输出：["我今天去了公园，看到了很多花。", "然后去了图书馆。"]
```

### 8.3 节点创建与关联建立

```
每个切割后的小段
    │
    ├─► 压缩（按目标字数 = min(重要性,1) × 原文）
    ├─► 提取关键词
    ├─► 创建记忆节点
    │
    ▼ 建立关联
        ├─► 与当前所有关注点：强度 = 1.0
        └─► 同一切割来源的节点之间：强度 = 0.5
```

**关联规则说明**：
- 与关注点关联：强度固定为1，不衰减（受关注点保护）
- 切割节点间关联：强度0.5，正常衰减，表示语义连贯性

### 8.4 从上下文到记忆网络

**完整流程**：
```
智能体对话
    │
    ▼ 上下文增长
    │
    ▼ 达到阈值（外部系统判断）
    │
    ▼ 语义切割 → 多个小段
    │
    ├─► 小段1 ──┐
    ├─► 小段2 ──┼──► 建立关联（强度0.5）
    ├─► 小段3 ──┤
    │           │
    ▼           ▼
与关注点建立关联（强度1.0）
    │
    ▼ 加入记忆网络
```

---

## 9. 节点生命周期（数学视角）

```
创建节点
    │
    ▼ 初始状态
┌─────────────────────────────────────┐
│ 内容 = 完整内容                      │
│ 初始关联强度 = 0.5                   │
│ 创建时间 = t₀                        │
│ 扫描次数 = 0                         │
└─────────────────────────────────────┘
    │
    ▼ 时间演化（每单位时间）
    
    重要性 = Σ sᵢ
    │
    ├─► 所有关联强度 ×= decay_rate
    ├─► 移除强度 < 0.01 的关联
    ├─► 更新内容（按重要性比例保留，目标字数 = min(重要性,1) × 原文）
    ├─► 更新短语/关键词
    ├─► 扫描次数 += 1
    │
    ▼ 事件触发
    
    收到相关内容 ──► 新关联建立（强度=0.5）──► 重要性增加
    
    │
    ▼ 终止条件
    
    重要性 = 0 ──► 节点遗忘（无关联）
    或
    内容压缩至空 ──► 节点遗忘
```

---

## 10. 待明确的数学定义

### 10.1 重要性公式

需要确定：

- [x] 函数形式：直接求和 `重要性 = Σsᵢ`
- [x] 取值范围：[0, ∞)
- [ ] 压缩公式：目标字数 = min(重要性, 1) × 原文

### 10.2 压缩过程的定义

#### 10.2.1 触发机制：基于对话Token累计

压缩循环由**智能体对话活动**驱动，而非定时器。

**核心变量**：

| 变量 | 符号 | 说明 |
|------|------|------|
| 对话最大Token数 | T_max | 系统允许的单次对话Token上限 |
| 最小启动压缩Token数 | T_trigger | 触发压缩申请的累计Token阈值 |

**触发流程**：

```
每次对话发生
    │
    ▼ 累加本次对话Token数到累计值 T_accumulated
    │
    ▼ 判断：T_accumulated ≥ T_trigger ?
    │
    ├─ 否 → 继续正常对话，不触发压缩
    │
    └─ 是 → 申请执行一次压缩
              │
              ▼ 重置 T_accumulated = 0
              │
              ▼ 将压缩任务加入系统队列
              │
              ▼ 所有智能体按排队顺序执行压缩
```

**关键设计**：
- 压缩**不是立即执行**，而是进入系统队列
- 系统中所有智能体**排队执行**压缩任务
- 每次压缩对每个节点计算重要性并执行压缩
- 随着对话持续进行，会进行很多轮次压缩
- **时间的影响蕴含在多次压缩中**，不需要显式的时间参数计算

#### 10.2.2 每次压缩的执行方式

- 内容按比例缩减（由重要性决定）
- 更新短语/关键词
- 关联强度衰减

#### 10.2.3 压缩的粒度与调度

**时间片调度机制**：

| 参数 | 值 | 说明 |
|------|-----|------|
| 时间片长度 | `time_slice` = 30秒 | 每个智能体的单次压缩时长 |

**调度流程**：
```
系统启动压缩
    │
    ▼ 分配时间片给智能体A（30秒）
    │
    ▼ 智能体A开始压缩其记忆节点
    │   （逐个节点串行处理）
    │
    ▼ 时间片结束（30秒到）
    │   无论处理了多少节点，立即暂停
    │
    ▼ 切换到智能体B，分配时间片
    │
    ▼ ...循环直到所有智能体处理完毕
```

**特性**：
- 压缩任务**可暂停、可随时停止**
- 最小粒度：**单个节点**
- 调度方式：时间片轮转，在智能体间**分时执行**
- 一个智能体的完整压缩可能分布在**多个时间片**内完成

**并行性**：
- 节点之间**无依赖**，理论上可并行处理
- 实际实现：小模型计算速度有限，采用**串行处理**

#### 10.2.4 扫描顺序与长期记忆（方案选择）

有两种实现方案，根据性能和资源需求选择：

##### 方案A：固定节点数（推荐）

**核心思想**：系统维护固定数量的节点槽位，新节点需要创建时，复用最不重要的节点槽位。

```
固定节点池（如10000个槽位）
    │
    ▼ 需要创建新节点
    │
    ▼ 找到当前最不重要的节点（重要性最低）
    │   （无需精确排序，随机采样估计即可）
    │
    ▼ 复用该槽位：
        - 清空原节点数据
        - 断开原节点关联（懒删除）
        - 生成新ID
        - 写入新节点数据
```

**优势**：
- 内存和存储占用固定，无上限风险
- 无需维护scan_count排序，性能稳定
- 旧连接指向旧ID，物理位置相同但ID变化，天然识别"遗忘"

**适用场景**：资源受限、长期运行的系统

##### 方案B：扫描次数排序

**扫描次数**（`scan_count`）：记录节点被压缩扫描的次数。

**扫描顺序规则**：
```
优先扫描 scan_count 小的节点
    │
    ▼ 次数少的节点先被处理
    │
    ▼ 处理完后 scan_count + 1
    │
    ▼ 下次扫描时排到后面
```

**性能优化**：
- 不需要绝对精确的排序
- 可采用概率数据结构（如Skip List）或随机采样估计
- 有限次扫描后统计上趋近一致即可

**长期记忆形成机制**：

| 扫描次数 | 效果 |
|----------|------|
| 少 | 容易被扫描到，容易被压缩 |
| 多 | 难以被扫描到，难以被压缩 |

**本质**：
- 被反复扫描的记忆 → 扫描次数增加 → 扫描间隔变长 → 压缩频率降低
- 形成**长期记忆**：不是不被遗忘，而是遗忘速度极慢

**适用场景**：节点数量可控、需要精确长期记忆区分的系统

#### 10.2.5 删除阈值机制

**直接删除条件**：
```
目标字数 = 原始字数 × min(重要性, 1)

如果 目标字数 < delete_threshold(10) → 直接删除节点
```

**作用**：
- 避免对过短内容调用小模型压缩
- 减少无效计算
- 快速清理碎片记忆

### 10.3 关联强度演化

**已确定**：
- ✅ 新关联初始强度：`link_initial_strength` = 0.5
- ✅ 关联强度衰减：每次压缩时乘以 `decay_rate`
- ✅ 关联断裂阈值：强度 < 0.01

### 10.4 内容压缩实现

#### 压缩算法：小模型语义压缩

**保留比例计算**：
```
目标字数 = 原始字数 × min(重要性, 1)
```

| 重要性 | 效果 |
|----------|------|
| 0 | 保留0%（直接删除节点/遗忘）|
| 0.5 | 保留50% |
| 1 | 保留100%（不压缩）|
| > 1 | 保留100%（受到保护）|

**压缩流程**：
```
原始文本
    │
    ▼ 计算目标字数 = len(原文) × min(重要性, 1)
    │
    ▼ 判断：目标字数 < delete_threshold(10) ?
    │
    ├─ 是 → 直接删除节点
    │
    └─ 否 → 调用内置小模型
            输入：原文 + 目标字数要求
            输出：压缩后文本
            │
            ▼ 替换节点内容
```

**删除阈值**：`delete_threshold` = 10（字符）

**小模型职责**：
- 理解原文语义
- 在指定字数限制下保留核心信息
- 返回简洁、连贯的压缩文本

**"不可理解"的量化定义**：
```
不可理解 ⇔ 目标字数 < delete_threshold(10)
```
- 当压缩后文字长度小于10个字符时，内容已无法承载有效语义
- 此时直接删除节点，而非进行无意义的压缩

### 10.5 边界情况处理

#### 节点删除后的关联处理

**懒删除策略**：
- 节点被删除时，**不主动清理**指向它的关联
- 当其他节点访问该关联时，发现目标节点不存在 → **自动删除该关联**

```
节点A → 关联 → [节点B（已删除）]
    │
    ▼ 访问关联时检测到节点B不存在
    │
    ▼ 自动删除节点A的这条关联
```

#### scan_count 上限

**无上限设计**：
- `scan_count` 会一直增长，不考虑上限
- 理论上，当节点非常多时，两次扫描同一节点的时间差可能达到**几十年**
- 这正是长期记忆的形成机制：扫描次数越高，被扫描频率越低

#### 小模型压缩失败处理

**重试机制**：
```
调用小模型压缩
    │
    ▼ 失败
    │
    ▼ 重试（最多15次）
    │
    ├─ 成功 → 更新节点内容
    │
    └─ 15次均失败 → 放弃本次压缩，保留原内容
```

---

## 11. 对比：数学化 vs 逻辑化

| 特性 | 数学化设计 | 逻辑化设计 |
|------|-----------|-----------|
| 状态变化 | 连续渐变 | 离散跳转 |
| 决策 | 公式计算 | if-else判断 |
| 遗忘 | 重要性=0的自然结果 | delete操作 |
| 关联等级 | 连续强度值 | 强/中/弱枚举 |
| 可预测性 | 可微分、可优化 | 硬编码规则 |

---

**最后更新**：2026-02-05

**记录人**：用户

**变更记录**：
- 新增：记忆的建立章节（第8节）
- 新增：语义切割机制（8.2、8.3）
- 新增：节点与关注点关联强度=1规则（8.3）
- 新增：切割节点间关联强度=0.5规则（8.3）
- 新增：外部系统触发判断说明（8.1）
- 新增：边界情况处理（10.5）
- 新增：节点删除后关联懒删除机制（10.5）
- 新增：小模型压缩失败重试15次机制（10.5）
- 新增：扫描次数与长期记忆机制（2、9、10.2.4）
- 新增：扫描顺序优先处理次数少的节点（10.2.4）
- 新增：关注点机制（第7节）
- 新增：关注点来源于回忆函数关键词（7.3）
- 新增：关键词要求：名词/代词，指代事物/领域（7.3）
- 更新：回忆函数增加 `depth` 参数，BFS搜索（6.1）
- 更新：回忆函数改为异步函数，后台线程执行（6.1）
- 新增：与现有方案的区别对比（1.2）
- 新增：回忆函数接口定义（第6节）
- 新增：时间片调度机制，每个智能体30秒（10.2.3）
- 新增：删除阈值机制（10.2.5、10.4）
- 新增：压缩粒度与调度机制（10.2.3）
- 新增：压缩可暂停、可随时停止（10.2.3）
- 新增：关联衰减速率变量及衰减机制（4.1、10.3）
- 新增：关联断裂阈值 0.01（4.1）
- 新增：小模型语义压缩方案（10.4）
- 新增：压缩比例计算公式（10.4）
- 新增：基于对话Token累计的压缩触发机制（10.2.1）
- 新增：压缩排队执行机制（10.2.1）
- 新增：强化机制（6.1-6.3）
- 修改：时间演化总览拆分为自然退化与强化干预（7.1-7.2）
- 新增：待明确问题分类（8.1-8.4）
- 重大更新：重新定义"重要性"数学化概念（3.1-3.3）
- 新增：数学化设计原则（第5节）
- 新增：节点生命周期数学视角（第6节）
- 新增：待明确的数学定义（第7节）
- 新增：数学化vs逻辑化对比（第8节）
- **重要修正**：时间不再参与重要性计算，仅作为元数据（3.1、5.3、7.1-7.2）
